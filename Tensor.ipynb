{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling2D, TimeDistributed, Conv1D, Conv2D, GlobalAveragePooling1D, MaxPool2D, Flatten, add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# silence tensorflow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# getting rid of the warning messages about optimizer graph\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num CPUs Available:  1\n",
      "Tensorflow version: 2.11.0\n",
      "Keras version: 2.11.0\n",
      "Using NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "# print Tensorflow and CUDA information\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    " \n",
    "if tf.test.gpu_device_name():\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    details = tf.config.experimental.get_device_details(gpu_devices[0])\n",
    "    name = details.get('device_name', 'Unknown GPU')\n",
    "    \n",
    "    print(f\"Using {name}\")\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vggish_params as params\n",
    "\n",
    "\n",
    "path = 'vggish_model.ckpt'\n",
    "\n",
    "class VGGish(tf.keras.Model):\n",
    "    def __init__(self, training=False):\n",
    "        super(VGGish, self).__init__()\n",
    "        self.training = training\n",
    "\n",
    "        # The VGG stack of alternating convolutions and max-pools.\n",
    "        self.conv1 = Conv2D(64, kernel_size=[3, 3], padding='same', activation=tf.nn.relu, trainable=self.training)\n",
    "        self.pool1 = MaxPool2D(pool_size=[2, 2], padding='same', trainable=self.training)\n",
    "        self.conv2 = Conv2D(128, kernel_size=[3, 3], padding='same', activation=tf.nn.relu, trainable=self.training)\n",
    "        self.pool2 = MaxPool2D(pool_size=[2, 2], padding='same', trainable=self.training)\n",
    "        self.conv3_1 = Conv2D(256, kernel_size=[3, 3], padding='same', activation=tf.nn.relu, trainable=self.training)\n",
    "        self.conv3_2 = Conv2D(256, kernel_size=[3, 3], padding='same', activation=tf.nn.relu, trainable=self.training)\n",
    "        self.pool3 = MaxPool2D(pool_size=[2, 2], padding='same', trainable=self.training)\n",
    "        self.conv4_1 = Conv2D(512, kernel_size=[3, 3], padding='same', activation=tf.nn.relu, trainable=self.training)\n",
    "        self.conv4_2 = Conv2D(512, kernel_size=[3, 3], padding='same', activation=tf.nn.relu, trainable=self.training)\n",
    "        self.pool4 = MaxPool2D(pool_size=[2, 2], padding='same', trainable=self.training)\n",
    "\n",
    "        # Flatten before entering fully-connected layers\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1_1 = Dense(4096, activation=tf.nn.relu, trainable=self.training)\n",
    "        self.fc1_2 = Dense(4096, activation=tf.nn.relu, trainable=self.training)\n",
    "        # The embedding layer.\n",
    "        self.fc2 = Dense(params.EMBEDDING_SIZE, activation=None, trainable=self.training)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        net = self.conv1(inputs)\n",
    "        net = self.pool1(net)\n",
    "        net = self.conv2(net)\n",
    "        net = self.pool2(net)\n",
    "        net = self.conv3_1(net)\n",
    "        net = self.conv3_2(net)\n",
    "        net = self.pool3(net)\n",
    "        net = self.conv4_1(net)\n",
    "        net = self.conv4_2(net)\n",
    "        net = self.pool4(net)\n",
    "\n",
    "        net = self.flatten(net)\n",
    "        net = self.fc1_1(net)\n",
    "        net = self.fc1_2(net)\n",
    "        net = self.fc2(net)\n",
    "        \n",
    "        return net\n",
    "\n",
    "    def load_vggish_slim_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Loads a pre-trained VGGish-compatible checkpoint.\"\"\"\n",
    "        self.load_weights(checkpoint_path)\n",
    "\n",
    "vggish = VGGish()\n",
    "vggish.load_vggish_slim_checkpoint(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vg_gish_classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vg_gish_2 (VGGish)          multiple                  72141184  \n",
      "                                                                 \n",
      " dense_9 (Dense)             multiple                  66048     \n",
      "                                                                 \n",
      " dense_10 (Dense)            multiple                  131328    \n",
      "                                                                 \n",
      " dense_11 (Dense)            multiple                  32896     \n",
      "                                                                 \n",
      " dense_12 (Dense)            multiple                  65664     \n",
      "                                                                 \n",
      " dense_13 (Dense)            multiple                  2580      \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72,439,700\n",
      "Trainable params: 298,516\n",
      "Non-trainable params: 72,141,184\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import vggish_input\n",
    "\n",
    "class VGGishClassifier(tf.keras.Model):\n",
    "    def __init__(self, vggish_model, num_classes):\n",
    "        super(VGGishClassifier, self).__init__()\n",
    "        self.vggish_model = vggish_model\n",
    "        self.dense1 = Dense(512, activation='relu')\n",
    "        self.dense2 = Dense(256, activation='relu')\n",
    "        self.dense3 = Dense(128, activation='relu')\n",
    "        self.skip1 = Dense(128, activation='relu')\n",
    "        self.dense4 = Dense(num_classes, activation='sigmoid')\n",
    "        self.dropout = Dropout(0.5)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.vggish_model(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        skip = self.skip1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = add([x, skip])\n",
    "        x = self.dense4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 20  # Set the number of classes as needed\n",
    "classifier_model = VGGishClassifier(vggish, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "classifier_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                         loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Prepare the input data and labels\n",
    "batch_size = 10\n",
    "num_frames = params.NUM_FRAMES\n",
    "num_bands = params.NUM_BANDS\n",
    "\n",
    "input_data = np.random.rand(batch_size, num_frames, num_bands, 1).astype(np.float32)\n",
    "# labels = np.random.randint(0, num_classes, size=(batch_size,))\n",
    "# force all labels to be the same\n",
    "labels = np.ones((batch_size,)) * 5\n",
    "\n",
    "classifier_model.build(input_shape=(None, params.NUM_FRAMES, params.NUM_BANDS, 1))\n",
    "classifier_model.summary()\n",
    "\n",
    "# Train the classifier model\n",
    "if False:\n",
    "    classifier_model.fit(input_data, labels, epochs=10)\n",
    "\n",
    "    predictions = classifier_model.predict(input_data)\n",
    "\n",
    "    print(f\"Predictions shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, Add\n",
    "# import Rescaling\n",
    "from keras.layers.preprocessing.image_preprocessing import Rescaling\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Layer, MultiHeadAttention, Dense, LayerNormalization, Dropout, Reshape, Add\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(d_model),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class VGGishTransformerClassifier(tf.keras.Model):\n",
    "    def __init__(self, vggish_model, num_classes, num_heads=4, ff_dim=512):\n",
    "        super(VGGishTransformerClassifier, self).__init__()\n",
    "        self.vggish_model = vggish_model\n",
    "        self.reshape = Reshape((1, 128))  # Reshaping the output to (batch_size, 1, 128)\n",
    "        self.transformer_block = TransformerBlock(d_model=128, num_heads=num_heads, ff_dim=ff_dim)\n",
    "        self.dense1 = Dense(128, activation='relu')\n",
    "        self.skip1 = Dense(128, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense2 = Dense(num_classes, activation='sigmoid')\n",
    "        self.dropout = Dropout(0.5)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.vggish_model(inputs)\n",
    "        x = self.reshape(x)\n",
    "        x = self.transformer_block(x)\n",
    "        x = tf.squeeze(x, axis=1)  # Squeezing the output back to (batch_size, 128)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        skip = self.skip1(x)\n",
    "        x = Add()([x, skip])\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000135_483840' '000139_119040' '000141_153600' '000144_30720'\n",
      " '000145_172800']\n",
      "['000308_61440' '000312_184320' '000319_145920' '000321_218880'\n",
      " '000327_88320']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# train csv path\n",
    "train_csv_path = 'openmic-2018/partitions/split01_train.csv'\n",
    "# test csv path\n",
    "test_csv_path = 'openmic-2018/partitions/split01_test.csv'\n",
    "\n",
    "# open csvs\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# convert to numpy arrays\n",
    "train_df = train_df.to_numpy()\n",
    "test_df = test_df.to_numpy()\n",
    "\n",
    "# make each a single list\n",
    "train_df = train_df.flatten()\n",
    "test_df = test_df.flatten()\n",
    "\n",
    "# print the first 5 rows of the train and test dataframes\n",
    "print(train_df[:5])\n",
    "print(test_df[:5])\n",
    "\n",
    "# only use the first 10% of each csv\n",
    "train_df = train_df[:int(len(train_df) * 1)]\n",
    "test_df = test_df[:int(len(test_df) * 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrograms shape: (149140, 96, 64, 1)\n",
      "Labels shape: (149140, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'spectrograms'\n",
    "labels_path = 'labels.csv'\n",
    "\n",
    "# Read the labels CSV file\n",
    "# ['filename' 'clarinet' 'flute' 'trumpet' 'saxophone' 'voice' 'accordion' 'ukulele' 'mallet_percussion' 'piano' 'guitar' 'mandolin' 'banjo' 'synthesizer' 'trombone' 'organ' 'drums' 'bass' 'cymbals' 'cello' 'violin']\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "\n",
    "# Get the list of all the filenames\n",
    "filenames = labels_df['filename'].values.tolist()\n",
    "\n",
    "# load the spectrograms and labels\n",
    "spectrograms_train = []\n",
    "labels_train = []\n",
    "\n",
    "spectrograms_test = []\n",
    "labels_test = []\n",
    "\n",
    "#check if pickle file exists\n",
    "if not os.path.isfile('pickle/spectrograms_train.pkl'):\n",
    "    \n",
    "    for filename in filenames:\n",
    "        # if the filename is not in the train or test dataframe, skip it\n",
    "        if filename not in train_df and filename not in test_df:\n",
    "            continue\n",
    "\n",
    "        # load the spectrogram\n",
    "        spectrogram = np.load(os.path.join(dataset_path, filename + '.npy'))\n",
    "\n",
    "        # the fist index is the filename, the next 20 are the labels and the last 20 are the masks\n",
    "        label = labels_df[labels_df['filename'] == filename].values.tolist()[0][1:21]\n",
    "        mask = labels_df[labels_df['filename'] == filename].values.tolist()[0][21:]\n",
    "\n",
    "        # threshold the labels\n",
    "        label = np.array(label) > 0.5\n",
    "\n",
    "        # make a pair of the spectrogram and the label\n",
    "        combined = list(zip(label, mask))\n",
    "\n",
    "        # append each second seperatly\n",
    "        if filename in train_df:\n",
    "            for i in range(10):\n",
    "                spectrograms_train.append(spectrogram[i])\n",
    "                labels_train.append(combined)\n",
    "        elif filename in test_df:\n",
    "            for i in range(10):\n",
    "                spectrograms_test.append(spectrogram[i])\n",
    "                labels_test.append(combined)\n",
    "        else:\n",
    "            continue\n",
    "            # print(f\"Filename {filename} not found in train or test dataframes\")\n",
    "\n",
    "    # convert the lists to numpy arrays\n",
    "    spectrograms_train = np.array(spectrograms_train)\n",
    "    labels_train = np.array(labels_train)\n",
    "\n",
    "    spectrograms_test = np.array(spectrograms_test)\n",
    "    labels_test = np.array(labels_test)\n",
    "\n",
    "    #spectrograms = spectrograms.reshape(spectrograms.shape[0], num_frames, num_bands, 1)\n",
    "    spectrograms_test = np.expand_dims(spectrograms_test, axis=-1)\n",
    "    spectrograms_train = np.expand_dims(spectrograms_train, axis=-1)\n",
    "\n",
    "    #pickle the spectrogram test and train data\n",
    "    pickle.dump(spectrograms_test, open('pickle/spectrograms_test.pkl', 'wb'))\n",
    "    pickle.dump(spectrograms_train, open('pickle/spectrograms_train.pkl', 'wb'))\n",
    "    #pickle the labels\n",
    "    pickle.dump(labels_test, open('pickle/labels_test.pkl', 'wb'))\n",
    "    pickle.dump(labels_train, open('pickle/labels_train.pkl', 'wb'))\n",
    "else:\n",
    "    #load the spectrogram test and train data\n",
    "    spectrograms_test = pickle.load(open('pickle/spectrograms_test.pkl', 'rb'))\n",
    "    spectrograms_train = pickle.load(open('pickle/spectrograms_train.pkl', 'rb'))\n",
    "    #load the labels\n",
    "    labels_test = pickle.load(open('pickle/labels_test.pkl', 'rb'))\n",
    "    labels_train = pickle.load(open('pickle/labels_train.pkl', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Spectrograms shape: {spectrograms_train.shape}\")\n",
    "print(f\"Labels shape: {labels_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 400ms/step\n",
      "(10, 128)\n"
     ]
    }
   ],
   "source": [
    "x = vggish.predict(spectrograms_train[:10])\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (149140, 96, 64, 1)\n",
      "y_train shape: (149140, 20, 2)\n",
      "X_test shape: (50840, 96, 64, 1)\n",
      "y_test shape: (50840, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def m_binary_crossentropy(y_true, y_pred):\n",
    "    # Separate labels and masks from y_true\n",
    "    labels = y_true[..., 0]\n",
    "    \n",
    "    # Compute the binary crossentropy\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    loss = bce(labels, y_pred)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def BCEp(y_true, y_pred, alpha=1.0, beta=0.0, gamma=-1.0):\n",
    "    # y_true and y_pred have shape (batch_size, 20)\n",
    "\n",
    "    # Get the ground truth for each label\n",
    "    y_true_labels = y_true[..., 0]\n",
    "\n",
    "    # Get the mask for each label\n",
    "    y_true_masks = y_true[..., 1]\n",
    "\n",
    "    # ground truth * log(prediction) + (1 - ground truth) * log(1 - prediction)\n",
    "    # unreduced_bce_func = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    # bce = unreduced_bce_func(y_true_labels, y_pred)\n",
    "\n",
    "    bce = y_true_labels * tf.math.log(y_pred) + (1 - y_true_labels) * tf.math.log(1 - y_pred)\n",
    "\n",
    "    # print shape\n",
    "    print(f\"bce shape: {bce.shape}\")\n",
    "\n",
    "    # mask the loss to zero for labels that are not present\n",
    "    masked_bce = bce * y_true_masks\n",
    "\n",
    "    # compute proportion of labels that are present\n",
    "    present_labels = tf.reduce_sum(y_true_masks, axis=1) / 20.0\n",
    "\n",
    "    # nomalize the present labels then divide by the number of labels\n",
    "    normalized_present_labels = (alpha * (present_labels ** gamma) + beta) / 20.0\n",
    " \n",
    "\n",
    "    print(f\"normalized_present_labels shape: {normalized_present_labels}\")\n",
    "\n",
    "    # reduce the loss\n",
    "    loss = tf.reduce_sum(masked_bce, axis=1) * normalized_present_labels\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def m_accuracy(y_true, y_pred):\n",
    "    # Separate labels and masks from y_true. The masks should be ignored.\n",
    "    labels = y_true[..., 0]\n",
    "    masks = y_true[..., 1]\n",
    "\n",
    "    # Threshold predictions to convert them to binary values (assuming 0.5 as the threshold)\n",
    "    binary_pred = tf.cast(tf.greater_equal(y_pred, 0.5), tf.float32)\n",
    "\n",
    "    # Compute the element-wise equality between binary_pred and y_true\n",
    "    correct_predictions = tf.cast(tf.equal(binary_pred, labels), tf.float32)\n",
    "\n",
    "    # multiply by the mask\n",
    "    correct_predictions = correct_predictions * masks\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = tf.reduce_sum(correct_predictions) / tf.reduce_sum(masks)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "@tf.function\n",
    "def m_f1(y_true, y_pred):\n",
    "    # Separate labels and masks from y_true\n",
    "    labels = y_true[..., 0]\n",
    "    masks = y_true[..., 1]\n",
    "\n",
    "    # Threshold predictions to convert them to binary values (assuming 0.5 as the threshold)\n",
    "    binary_pred = tf.cast(tf.greater_equal(y_pred, 0.5), tf.float32)\n",
    "\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    true_positives = tf.reduce_sum(binary_pred * labels * masks)\n",
    "    false_positives = tf.reduce_sum(binary_pred * (1 - labels) * masks)\n",
    "    false_negatives = tf.reduce_sum((1 - binary_pred) * labels * masks)\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = true_positives / (true_positives + false_positives + 1e-8)\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "X_train = spectrograms_train\n",
    "y_train = labels_train\n",
    "\n",
    "X_test = spectrograms_test\n",
    "y_test = labels_test\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4661/4661 [==============================] - 17s 4ms/step\n",
      "1589/1589 [==============================] - 6s 4ms/step\n",
      "vgg_X_train shape: (149140, 128)\n",
      "vgg_X_test shape: (50840, 128)\n"
     ]
    }
   ],
   "source": [
    "# vgg_X_train = vggish.predict(X_train)\n",
    "# vgg_X_test = vggish.predict(X_test)\n",
    "\n",
    "# print(f\"vgg_X_train shape: {vgg_X_train.shape}\")\n",
    "# print(f\"vgg_X_test shape: {vgg_X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle the vgg_X_test and vgg_X_train data\n",
    "# pickle.dump(vgg_X_test, open('pickle/vgg_X_test.pkl', 'wb'))\n",
    "# pickle.dump(vgg_X_train, open('pickle/vgg_X_train.pkl', 'wb'))\n",
    "\n",
    "#load the vgg_X_test and vgg_X_train data\n",
    "vgg_X_test = pickle.load(open('pickle/vgg_X_test.pkl', 'rb'))\n",
    "vgg_X_train = pickle.load(open('pickle/vgg_X_train.pkl', 'rb'))\n",
    "print(f\"vgg_X_train shape: {vgg_X_train.shape}\")\n",
    "print(f\"vgg_X_test shape: {vgg_X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03333668  0.01239415  0.02070752  0.02415475 -0.0193878  -0.00188267\n",
      "  0.03266878 -0.03517116  0.00617049 -0.01298855  0.03187737 -0.03661109\n",
      " -0.02493214  0.03001267  0.07322174 -0.03755243 -0.00109528  0.01354072\n",
      "  0.01612341 -0.03816056 -0.0046137  -0.05460477  0.02413895  0.05334652\n",
      " -0.00412781 -0.01510066  0.00227696 -0.01563813 -0.04267376  0.04319832\n",
      " -0.02992732 -0.00603651  0.02305216 -0.05009295  0.02032151 -0.02937008\n",
      "  0.00359633 -0.01117912 -0.02908186 -0.01970048 -0.00572515  0.03168309\n",
      " -0.01354233 -0.05525997 -0.03986001  0.01863924 -0.01167305 -0.01035044\n",
      "  0.00624337 -0.0528811   0.03228129 -0.02697513 -0.01131375 -0.00473073\n",
      " -0.01244172 -0.02195379 -0.00179833 -0.00452174 -0.03076958  0.04686664\n",
      " -0.00531481  0.00782339 -0.0277487  -0.01618263 -0.02501273  0.00779569\n",
      "  0.02077115 -0.00990653  0.02501084  0.00406783  0.00570872 -0.02781973\n",
      " -0.00594668 -0.02384339 -0.03762325 -0.00755598  0.03127922 -0.00738188\n",
      "  0.02148581 -0.01868164  0.03107121 -0.03537071  0.01737864  0.0002741\n",
      " -0.01038209  0.03957249  0.00956871  0.01464908  0.02848819  0.01155826\n",
      "  0.02387971  0.00838365 -0.00788632 -0.01991426 -0.03257589  0.02841319\n",
      "  0.02915363  0.02823068  0.01343086 -0.01370855  0.04195113 -0.04915098\n",
      " -0.00029486  0.03891692 -0.0084289  -0.00617971 -0.01533315 -0.05408863\n",
      "  0.01900458 -0.01740983  0.00762203  0.01369481  0.02129301  0.00817358\n",
      "  0.00507763 -0.00362907 -0.00180095  0.03549657  0.00230059  0.03911489\n",
      "  0.00574327  0.00429505  0.05050755 -0.04182635 -0.00047596  0.06730574\n",
      "  0.04062844  0.01917725]\n"
     ]
    }
   ],
   "source": [
    "print (vgg_X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the data such that it can be fed into the model in this format (samples_num, freq_bins, time_steps, 1), where each sample is 10 seconds long\n",
    "vgg_X_train_tensor = vgg_X_train.reshape((int(vgg_X_train.shape[0]/10),10, vgg_X_train.shape[1]))\n",
    "vgg_X_test_tensor = vgg_X_test.reshape((int(vgg_X_test.shape[0]/10),10, vgg_X_test.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_X_train_tensor shape: (14914, 10, 128)\n",
      "vgg_X_test_tensor shape: (5084, 10, 128)\n"
     ]
    }
   ],
   "source": [
    "print(f\"vgg_X_train_tensor shape: {vgg_X_train_tensor.shape}\")\n",
    "print(f\"vgg_X_test_tensor shape: {vgg_X_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pytorch model\n",
    "import torch\n",
    "model = torch.load('model_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20])\n"
     ]
    }
   ],
   "source": [
    "#evalueate the model on the first 10 samples in the training set\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(torch.from_numpy(vgg_X_train_tensor[0:10]).float().to(device))\n",
    "    print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet:\n",
    "    def __init__(self, input_shape, num_filters):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.classifier = model\n",
    "        self.embedding = vggish\n",
    "        self.build()\n",
    "\n",
    "    def conv_block(self, input, num_filters):\n",
    "        x = Conv2D(num_filters, 3, padding=\"same\", activation=\"relu\")(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(num_filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        return x\n",
    "\n",
    "    def encoder_block(self, input, num_filters):\n",
    "        x = self.conv_block(input, num_filters)\n",
    "        p = MaxPooling2D((2, 2))(x)\n",
    "        return x, p\n",
    "\n",
    "    def decoder_block(self, input, skip_features, num_filters):\n",
    "        x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "        x = concatenate([x, skip_features], axis=3)\n",
    "        x = self.conv_block(x, num_filters)\n",
    "        return x\n",
    "\n",
    "    def build(self):\n",
    "\n",
    "        #\n",
    "\n",
    "\n",
    "        # Input\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "\n",
    "        # Encoder\n",
    "        e1, p1 = self.encoder_block(inputs, self.num_filters)\n",
    "        e2, p2 = self.encoder_block(p1, self.num_filters*2)\n",
    "        e3, p3 = self.encoder_block(p2, self.num_filters*4)\n",
    "        e4, p4 = self.encoder_block(p3, self.num_filters*8)\n",
    "\n",
    "        # Bridge\n",
    "        b1 = self.conv_block(p4, self.num_filters*16)\n",
    "\n",
    "        # Decoder\n",
    "        d1 = self.decoder_block(b1, e4, self.num_filters*8)\n",
    "        d2 = self.decoder_block(d1, e3, self.num_filters*4)\n",
    "        d3 = self.decoder_block(d2, e2, self.num_filters*2)\n",
    "        d4 = self.decoder_block(d3, e1, self.num_filters)\n",
    "\n",
    "        # Output\n",
    "        outputs = Conv2D(1, (1, 1), activation='sigmoid')(d4)\n",
    "\n",
    "        # Model\n",
    "        self.model = Model(inputs, outputs)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def compile(self, optimizer, loss, metrics=None):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vg_gish_transformer_classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vg_gish (VGGish)            multiple                  72141184  \n",
      "                                                                 \n",
      " reshape (Reshape)           multiple                  0         \n",
      "                                                                 \n",
      " transformer_block (Transfor  multiple                 396032    \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " dense_10 (Dense)            multiple                  16512     \n",
      "                                                                 \n",
      " dense_11 (Dense)            multiple                  16512     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            multiple                  2580      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72,572,820\n",
      "Trainable params: 431,636\n",
      "Non-trainable params: 72,141,184\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_21' defined at (most recent call last):\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1254975/3869318690.py\", line 17, in <module>\n      history = classifier_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=32, batch_size=128)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_21'\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall_21}}]] [Op:__inference_train_function_4802]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m classifier_model\u001b[39m.\u001b[39mbuild(input_shape\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, params\u001b[39m.\u001b[39mNUM_FRAMES, params\u001b[39m.\u001b[39mNUM_BANDS, \u001b[39m1\u001b[39m))\n\u001b[1;32m     15\u001b[0m classifier_model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m---> 17\u001b[0m history \u001b[39m=\u001b[39m classifier_model\u001b[39m.\u001b[39;49mfit(X_train, y_train, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test), epochs\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m predictions \u001b[39m=\u001b[39m classifier_model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredictions shape: \u001b[39m\u001b[39m{\u001b[39;00mpredictions\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/podElias/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/podElias/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_21' defined at (most recent call last):\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1254975/3869318690.py\", line 17, in <module>\n      history = classifier_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=32, batch_size=128)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/users/emann/.conda/envs/podElias/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_21'\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall_21}}]] [Op:__inference_train_function_4802]"
     ]
    }
   ],
   "source": [
    "# import f1\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "num_classes = 20  # Set the number of classes as needed\n",
    "# classifier_model = VGGishClassifier(vggish, num_classes)\n",
    "classifier_model = VGGishTransformerClassifier(vggish, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "classifier_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                         loss=m_binary_crossentropy,\n",
    "                         metrics=[m_accuracy, m_f1])\n",
    "\n",
    "classifier_model.build(input_shape=(None, params.NUM_FRAMES, params.NUM_BANDS, 1))\n",
    "\n",
    "classifier_model.summary()\n",
    "\n",
    "history = classifier_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=32, batch_size=128)\n",
    "\n",
    "predictions = classifier_model.predict(X_test)\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# print the first prediction rounded to 2 decimal places\n",
    "print(f\"First prediction: {np.round(predictions[0], 2)}\")\n",
    "print(f\"First label:      {y_test[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
