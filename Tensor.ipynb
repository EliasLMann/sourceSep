{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling2D, TimeDistributed, Conv1D, Conv2D, GlobalAveragePooling1D, MaxPool2D, Flatten, add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# silence tensorflow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# getting rid of the warning messages about optimizer graph\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num CPUs Available:  1\n",
      "Tensorflow version: 2.11.0\n",
      "Keras version: 2.11.0\n",
      "Using NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 14:07:06.697373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 14:07:06.709154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 14:07:06.709213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 14:07:07.595607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 14:07:07.595673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-05-09 14:07:07.595801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-09 14:07:07.596159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 5413 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-05-09 14:07:07.601656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "# print Tensorflow and CUDA information\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    " \n",
    "if tf.test.gpu_device_name():\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    details = tf.config.experimental.get_device_details(gpu_devices[0])\n",
    "    name = details.get('device_name', 'Unknown GPU')\n",
    "    \n",
    "    print(f\"Using {name}\")\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vggish_params as params\n",
    "\n",
    "\n",
    "path = 'vggish_model.ckpt'\n",
    "\n",
    "class VGGish(tf.keras.Model):\n",
    "    def __init__(self, training=False):\n",
    "        super(VGGish, self).__init__()\n",
    "        self.training = training\n",
    "\n",
    "        # The VGG stack of alternating convolutions and max-pools.\n",
    "        self.conv1 = Conv2D(64, kernel_size=[3, 3], padding='same', activation=tf.nn.relu, trainable=self.training)\n",
    "        self.pool1 = MaxPool2D(pool_size=[2, 2], padding='same', trainable=self.training)\n",
    "        self.conv2 = Conv2D(128, kernel_size=[3, 3], padding='same', activation=tf.nn.relu, trainable=self.training)\n",
    "        self.pool2 = MaxPool2D(pool_size=[2, 2], padding='same', trainable=self.training)\n",
    "        self.conv3_1 = Conv2D(256, kernel_size=[3, 3], padding='same', activation=tf.nn.relu, trainable=self.training)\n",
    "        self.conv3_2 = Conv2D(256, kernel_size=[3, 3], padding='same', activation=tf.nn.relu, trainable=self.training)\n",
    "        self.pool3 = MaxPool2D(pool_size=[2, 2], padding='same', trainable=self.training)\n",
    "        self.conv4_1 = Conv2D(512, kernel_size=[3, 3], padding='same', activation=tf.nn.relu, trainable=self.training)\n",
    "        self.conv4_2 = Conv2D(512, kernel_size=[3, 3], padding='same', activation=tf.nn.relu, trainable=self.training)\n",
    "        self.pool4 = MaxPool2D(pool_size=[2, 2], padding='same', trainable=self.training)\n",
    "\n",
    "        # Flatten before entering fully-connected layers\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1_1 = Dense(4096, activation=tf.nn.relu, trainable=self.training)\n",
    "        self.fc1_2 = Dense(4096, activation=tf.nn.relu, trainable=self.training)\n",
    "        # The embedding layer.\n",
    "        self.fc2 = Dense(params.EMBEDDING_SIZE, activation=None, trainable=self.training)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        net = self.conv1(inputs)\n",
    "        net = self.pool1(net)\n",
    "        net = self.conv2(net)\n",
    "        net = self.pool2(net)\n",
    "        net = self.conv3_1(net)\n",
    "        net = self.conv3_2(net)\n",
    "        net = self.pool3(net)\n",
    "        net = self.conv4_1(net)\n",
    "        net = self.conv4_2(net)\n",
    "        net = self.pool4(net)\n",
    "\n",
    "        net = self.flatten(net)\n",
    "        net = self.fc1_1(net)\n",
    "        net = self.fc1_2(net)\n",
    "        net = self.fc2(net)\n",
    "        \n",
    "        return net\n",
    "\n",
    "    def load_vggish_slim_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Loads a pre-trained VGGish-compatible checkpoint.\"\"\"\n",
    "        self.load_weights(checkpoint_path)\n",
    "\n",
    "vggish = VGGish()\n",
    "vggish.load_vggish_slim_checkpoint(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vg_gish_classifier_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vg_gish_1 (VGGish)          multiple                  72141184  \n",
      "                                                                 \n",
      " dense_16 (Dense)            multiple                  66048     \n",
      "                                                                 \n",
      " dense_17 (Dense)            multiple                  131328    \n",
      "                                                                 \n",
      " dense_18 (Dense)            multiple                  32896     \n",
      "                                                                 \n",
      " dense_19 (Dense)            multiple                  65664     \n",
      "                                                                 \n",
      " dense_20 (Dense)            multiple                  2580      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72,439,700\n",
      "Trainable params: 298,516\n",
      "Non-trainable params: 72,141,184\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import vggish_input\n",
    "\n",
    "class VGGishClassifier(tf.keras.Model):\n",
    "    def __init__(self, vggish_model, num_classes):\n",
    "        super(VGGishClassifier, self).__init__()\n",
    "        self.vggish_model = vggish_model\n",
    "        self.dense1 = Dense(512, activation='relu')\n",
    "        self.dense2 = Dense(256, activation='relu')\n",
    "        self.dense3 = Dense(128, activation='relu')\n",
    "        self.skip1 = Dense(128, activation='relu')\n",
    "        self.dense4 = Dense(num_classes, activation='sigmoid')\n",
    "        self.dropout = Dropout(0.5)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.vggish_model(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        skip = self.skip1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = add([x, skip])\n",
    "        x = self.dense4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 20  # Set the number of classes as needed\n",
    "classifier_model = VGGishClassifier(vggish, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "classifier_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                         loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Prepare the input data and labels\n",
    "batch_size = 10\n",
    "num_frames = params.NUM_FRAMES\n",
    "num_bands = params.NUM_BANDS\n",
    "\n",
    "input_data = np.random.rand(batch_size, num_frames, num_bands, 1).astype(np.float32)\n",
    "# labels = np.random.randint(0, num_classes, size=(batch_size,))\n",
    "# force all labels to be the same\n",
    "labels = np.ones((batch_size,)) * 5\n",
    "\n",
    "classifier_model.build(input_shape=(None, params.NUM_FRAMES, params.NUM_BANDS, 1))\n",
    "classifier_model.summary()\n",
    "\n",
    "# Train the classifier model\n",
    "if False:\n",
    "    classifier_model.fit(input_data, labels, epochs=10)\n",
    "\n",
    "    predictions = classifier_model.predict(input_data)\n",
    "\n",
    "    print(f\"Predictions shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, Add\n",
    "# import Rescaling\n",
    "from keras.layers.preprocessing.image_preprocessing import Rescaling\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Layer, MultiHeadAttention, Dense, LayerNormalization, Dropout, Reshape, Add\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(d_model),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class VGGishTransformerClassifier(tf.keras.Model):\n",
    "    def __init__(self, vggish_model, num_classes, num_heads=4, ff_dim=512):\n",
    "        super(VGGishTransformerClassifier, self).__init__()\n",
    "        self.vggish_model = vggish_model\n",
    "        self.reshape = Reshape((1, 128))  # Reshaping the output to (batch_size, 1, 128)\n",
    "        self.transformer_block = TransformerBlock(d_model=128, num_heads=num_heads, ff_dim=ff_dim)\n",
    "        self.dense1 = Dense(128, activation='relu')\n",
    "        self.skip1 = Dense(128, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense2 = Dense(num_classes, activation='sigmoid')\n",
    "        self.dropout = Dropout(0.5)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.vggish_model(inputs)\n",
    "        x = self.reshape(x)\n",
    "        x = self.transformer_block(x)\n",
    "        x = tf.squeeze(x, axis=1)  # Squeezing the output back to (batch_size, 128)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        skip = self.skip1(x)\n",
    "        x = Add()([x, skip])\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000135_483840' '000139_119040' '000141_153600' '000144_30720'\n",
      " '000145_172800']\n",
      "['000308_61440' '000312_184320' '000319_145920' '000321_218880'\n",
      " '000327_88320']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# train csv path\n",
    "train_csv_path = 'openmic-2018/partitions/split01_train.csv'\n",
    "# test csv path\n",
    "test_csv_path = 'openmic-2018/partitions/split01_test.csv'\n",
    "\n",
    "# open csvs\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# convert to numpy arrays\n",
    "train_df = train_df.to_numpy()\n",
    "test_df = test_df.to_numpy()\n",
    "\n",
    "# make each a single list\n",
    "train_df = train_df.flatten()\n",
    "test_df = test_df.flatten()\n",
    "\n",
    "# print the first 5 rows of the train and test dataframes\n",
    "print(train_df[:5])\n",
    "print(test_df[:5])\n",
    "\n",
    "# only use the first 10% of each csv\n",
    "train_df = train_df[:int(len(train_df) * 0.15)]\n",
    "test_df = test_df[:int(len(test_df) * 0.15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrograms shape: (22370, 96, 64, 1)\n",
      "Labels shape: (22370, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'spectrograms'\n",
    "labels_path = 'labels.csv'\n",
    "\n",
    "# Read the labels CSV file\n",
    "# ['filename' 'clarinet' 'flute' 'trumpet' 'saxophone' 'voice' 'accordion' 'ukulele' 'mallet_percussion' 'piano' 'guitar' 'mandolin' 'banjo' 'synthesizer' 'trombone' 'organ' 'drums' 'bass' 'cymbals' 'cello' 'violin']\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "\n",
    "# Get the list of all the filenames\n",
    "filenames = labels_df['filename'].values.tolist()\n",
    "\n",
    "# load the spectrograms and labels\n",
    "spectrograms_train = []\n",
    "labels_train = []\n",
    "\n",
    "spectrograms_test = []\n",
    "labels_test = []\n",
    "\n",
    "for filename in filenames:\n",
    "    # if the filename is not in the train or test dataframe, skip it\n",
    "    if filename not in train_df and filename not in test_df:\n",
    "        continue\n",
    "\n",
    "    # load the spectrogram\n",
    "    spectrogram = np.load(os.path.join(dataset_path, filename + '.npy'))\n",
    "\n",
    "    # the fist index is the filename, the next 20 are the labels and the last 20 are the masks\n",
    "    label = labels_df[labels_df['filename'] == filename].values.tolist()[0][1:21]\n",
    "    mask = labels_df[labels_df['filename'] == filename].values.tolist()[0][21:]\n",
    "\n",
    "    # make a pair of the spectrogram and the label\n",
    "    combined = list(zip(label, mask))\n",
    "\n",
    "    # append each second seperatly\n",
    "    if filename in train_df:\n",
    "        for i in range(10):\n",
    "            spectrograms_train.append(spectrogram[i])\n",
    "            labels_train.append(combined)\n",
    "    elif filename in test_df:\n",
    "        for i in range(10):\n",
    "            spectrograms_test.append(spectrogram[i])\n",
    "            labels_test.append(combined)\n",
    "    else:\n",
    "        continue\n",
    "        # print(f\"Filename {filename} not found in train or test dataframes\")\n",
    "\n",
    "# convert the lists to numpy arrays\n",
    "spectrograms_train = np.array(spectrograms_train)\n",
    "labels_train = np.array(labels_train)\n",
    "\n",
    "spectrograms_test = np.array(spectrograms_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "#spectrograms = spectrograms.reshape(spectrograms.shape[0], num_frames, num_bands, 1)\n",
    "spectrograms_test = np.expand_dims(spectrograms_test, axis=-1)\n",
    "spectrograms_train = np.expand_dims(spectrograms_train, axis=-1)\n",
    "\n",
    "\n",
    "print(f\"Spectrograms shape: {spectrograms_train.shape}\")\n",
    "print(f\"Labels shape: {labels_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (22370, 96, 64, 1)\n",
      "y_train shape: (22370, 20, 2)\n",
      "X_test shape: (7620, 96, 64, 1)\n",
      "y_test shape: (7620, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "# custom partial binary crossentropy loss function\n",
    "@tf.function\n",
    "def partial_mean_squared_error(y_true, y_pred):\n",
    "    # y_true and y_pred are tensors with shape (batch_size, 20, 2) and (batch_size, 20) respectively\n",
    "    \n",
    "    # Separate labels and masks from y_true\n",
    "    labels = y_true[..., 0]\n",
    "    masks = y_true[..., 1]\n",
    "\n",
    "    # if the label is >.5 set it to 1, otherwise set it to 0\n",
    "    labels = tf.cast(tf.greater_equal(labels, 0.5), tf.float32)\n",
    "    \n",
    "    # Compute the squared error between y_pred and labels\n",
    "    squared_error = tf.square(labels - y_pred)\n",
    "    \n",
    "    # Apply the mask to the squared error\n",
    "    masked_squared_error = squared_error * masks\n",
    "    \n",
    "    # Compute the mean of the masked squared error\n",
    "    loss = tf.reduce_sum(masked_squared_error) / tf.reduce_sum(masks)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def unmasked_accuracy(y_true, y_pred):\n",
    "    # Separate labels and masks from y_true. The masks should be ignored.\n",
    "    labels = y_true[..., 0]\n",
    "\n",
    "    # Threshold predictions to convert them to binary values (assuming 0.5 as the threshold)\n",
    "    binary_pred = tf.cast(tf.greater_equal(y_pred, 0.5), tf.float32)\n",
    "\n",
    "    # Threshold the labels to convert them to binary values\n",
    "    binary_true = tf.cast(tf.greater_equal(labels, 0.5), tf.float32)\n",
    "\n",
    "    # Compute the element-wise equality between binary_pred and y_true\n",
    "    correct_predictions = tf.cast(tf.equal(binary_pred, binary_true), tf.float32)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = tf.reduce_mean(correct_predictions)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "@tf.function\n",
    "def unmasked_f1_score(y_true, y_pred):\n",
    "    # Separate labels and masks from y_true\n",
    "    labels = y_true[..., 0]\n",
    "\n",
    "    # Threshold predictions to convert them to binary values (assuming 0.5 as the threshold)\n",
    "    binary_pred = tf.cast(tf.greater_equal(y_pred, 0.5), tf.float32)\n",
    "\n",
    "    # Threshold the labels to convert them to binary values\n",
    "    binary_true = tf.cast(tf.greater_equal(labels, 0.5), tf.float32)\n",
    "\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    true_positives = tf.reduce_sum(binary_pred * binary_true)\n",
    "    false_positives = tf.reduce_sum(binary_pred * (1 - binary_true))\n",
    "    false_negatives = tf.reduce_sum((1 - binary_pred) * binary_true)\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = true_positives / (true_positives + false_positives + 1e-8)\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "X_train = spectrograms_train\n",
    "y_train = labels_train\n",
    "\n",
    "X_test = spectrograms_test\n",
    "y_test = labels_test\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vg_gish_transformer_classifier_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vg_gish_1 (VGGish)          multiple                  72141184  \n",
      "                                                                 \n",
      " reshape_7 (Reshape)         multiple                  0         \n",
      "                                                                 \n",
      " transformer_block_7 (Transf  multiple                 396032    \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " dense_53 (Dense)            multiple                  16512     \n",
      "                                                                 \n",
      " dense_54 (Dense)            multiple                  16512     \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " dense_55 (Dense)            multiple                  2580      \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72,572,820\n",
      "Trainable params: 431,636\n",
      "Non-trainable params: 72,141,184\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "175/175 [==============================] - 12s 49ms/step - loss: 0.2170 - unmasked_accuracy: 0.6434 - unmasked_f1_score: 0.1210 - val_loss: 0.2047 - val_unmasked_accuracy: 0.6815 - val_unmasked_f1_score: 0.1321\n",
      "Epoch 2/32\n",
      "175/175 [==============================] - 8s 45ms/step - loss: 0.1975 - unmasked_accuracy: 0.6927 - unmasked_f1_score: 0.1457 - val_loss: 0.2024 - val_unmasked_accuracy: 0.6516 - val_unmasked_f1_score: 0.1328\n",
      "Epoch 3/32\n",
      "175/175 [==============================] - 8s 45ms/step - loss: 0.1919 - unmasked_accuracy: 0.7043 - unmasked_f1_score: 0.1528 - val_loss: 0.2036 - val_unmasked_accuracy: 0.6636 - val_unmasked_f1_score: 0.1449\n",
      "Epoch 4/32\n",
      "175/175 [==============================] - 8s 46ms/step - loss: 0.1892 - unmasked_accuracy: 0.7014 - unmasked_f1_score: 0.1553 - val_loss: 0.1992 - val_unmasked_accuracy: 0.6788 - val_unmasked_f1_score: 0.1491\n",
      "Epoch 5/32\n",
      "175/175 [==============================] - 8s 45ms/step - loss: 0.1876 - unmasked_accuracy: 0.7028 - unmasked_f1_score: 0.1585 - val_loss: 0.1981 - val_unmasked_accuracy: 0.6957 - val_unmasked_f1_score: 0.1503\n",
      "Epoch 6/32\n",
      "175/175 [==============================] - 8s 46ms/step - loss: 0.1860 - unmasked_accuracy: 0.7003 - unmasked_f1_score: 0.1598 - val_loss: 0.1954 - val_unmasked_accuracy: 0.6991 - val_unmasked_f1_score: 0.1517\n",
      "Epoch 7/32\n",
      "175/175 [==============================] - 8s 46ms/step - loss: 0.1832 - unmasked_accuracy: 0.7093 - unmasked_f1_score: 0.1652 - val_loss: 0.1932 - val_unmasked_accuracy: 0.7073 - val_unmasked_f1_score: 0.1575\n",
      "Epoch 8/32\n",
      "175/175 [==============================] - 8s 46ms/step - loss: 0.1813 - unmasked_accuracy: 0.6987 - unmasked_f1_score: 0.1655 - val_loss: 0.1994 - val_unmasked_accuracy: 0.6775 - val_unmasked_f1_score: 0.1493\n",
      "Epoch 9/32\n",
      "175/175 [==============================] - 8s 46ms/step - loss: 0.1800 - unmasked_accuracy: 0.7107 - unmasked_f1_score: 0.1670 - val_loss: 0.1957 - val_unmasked_accuracy: 0.6610 - val_unmasked_f1_score: 0.1489\n",
      "Epoch 10/32\n",
      "175/175 [==============================] - ETA: 0s - loss: 0.1795 - unmasked_accuracy: 0.7038 - unmasked_f1_score: 0.1680"
     ]
    }
   ],
   "source": [
    "# import f1\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "num_classes = 20  # Set the number of classes as needed\n",
    "# classifier_model = VGGishClassifier(vggish, num_classes)\n",
    "classifier_model = VGGishTransformerClassifier(vggish, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "classifier_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                         loss=partial_mean_squared_error,\n",
    "                         metrics=[unmasked_accuracy, unmasked_f1_score])\n",
    "\n",
    "classifier_model.build(input_shape=(None, params.NUM_FRAMES, params.NUM_BANDS, 1))\n",
    "\n",
    "classifier_model.summary()\n",
    "\n",
    "history = classifier_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=32, batch_size=128)\n",
    "\n",
    "predictions = classifier_model.predict(X_test)\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# print the first prediction rounded to 2 decimal places\n",
    "print(f\"First prediction: {np.round(predictions[5], 2)}\")\n",
    "print(f\"First label:      {y_test[5][0]}\")\n",
    "print(f\"First mask:       {y_test[5][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
