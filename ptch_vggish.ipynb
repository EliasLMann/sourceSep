{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvggish import vggish, vggish_input\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import mir_eval\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000135_483840' '000139_119040' '000141_153600' '000144_30720'\n",
      " '000145_172800']\n",
      "['000308_61440' '000312_184320' '000319_145920' '000321_218880'\n",
      " '000327_88320']\n"
     ]
    }
   ],
   "source": [
    "# train csv path\n",
    "train_csv_path = 'openmic-2018/partitions/split01_train.csv'\n",
    "# test csv path\n",
    "test_csv_path = 'openmic-2018/partitions/split01_test.csv'\n",
    "\n",
    "# open csvs\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# convert to numpy arrays\n",
    "train_df = train_df.to_numpy()\n",
    "test_df = test_df.to_numpy()\n",
    "\n",
    "# make each a single list\n",
    "\n",
    "train_df = train_df.flatten()\n",
    "test_df = test_df.flatten()\n",
    "\n",
    "# print the first 5 rows of the train and test dataframes\n",
    "print(train_df[:5])\n",
    "print(test_df[:5])\n",
    "\n",
    "# only use the first 10% of each csv\n",
    "train_df = train_df[:int(len(train_df) * 1)]\n",
    "test_df = test_df[:int(len(test_df) * 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrograms shape: (14914, 10, 96, 64)\n",
      "Labels shape: (14914, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'spectrograms'\n",
    "labels_path = 'labels.csv'\n",
    "\n",
    "# Read the labels CSV file\n",
    "# ['filename' 'clarinet' 'flute' 'trumpet' 'saxophone' 'voice' 'accordion' 'ukulele' 'mallet_percussion' 'piano' 'guitar' 'mandolin' 'banjo' 'synthesizer' 'trombone' 'organ' 'drums' 'bass' 'cymbals' 'cello' 'violin']\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "\n",
    "# Get the list of all thae filenames\n",
    "filenames = labels_df['filename'].values.tolist()\n",
    "\n",
    "# load the spectrograms and labels\n",
    "spectrograms_train = []\n",
    "labels_train = []\n",
    "\n",
    "spectrograms_test = []\n",
    "labels_test = []\n",
    "\n",
    "#check if pickle file exists\n",
    "if not os.path.isfile('pickle/spectrograms_train.pkl'):\n",
    "    \n",
    "    for filename in filenames:\n",
    "        # if the filename is not in the train or test dataframe, skip it\n",
    "        if filename not in train_df and filename not in test_df:\n",
    "            continue\n",
    "\n",
    "        # load the spectrogram\n",
    "        spectrogram = np.load(os.path.join(dataset_path, filename + '.npy'))\n",
    "\n",
    "        # the fist index is the filename, the next 20 are the labels and the last 20 are the masks\n",
    "        label = labels_df[labels_df['filename'] == filename].values.tolist()[0][1:21]\n",
    "        mask = labels_df[labels_df['filename'] == filename].values.tolist()[0][21:]\n",
    "\n",
    "        # threshold the labels\n",
    "        label = np.array(label) > 0.5\n",
    "\n",
    "        # make a pair of the spectrogram and the label\n",
    "        combined = list(zip(label, mask))\n",
    "\n",
    "        # append each second seperatly\n",
    "        if filename in train_df:\n",
    "            spectrograms_train.append(spectrogram)\n",
    "            labels_train.append(combined)\n",
    "        elif filename in test_df:\n",
    "            spectrograms_test.append(spectrogram)\n",
    "            labels_test.append(combined)\n",
    "        else:\n",
    "            continue\n",
    "            # print(f\"Filename {filename} not found in train or test dataframes\")\n",
    "\n",
    "    # convert the lists to numpy arrays\n",
    "    spectrograms_train = np.array(spectrograms_train)\n",
    "    labels_train = np.array(labels_train)\n",
    "\n",
    "    spectrograms_test = np.array(spectrograms_test)\n",
    "    labels_test = np.array(labels_test)\n",
    "\n",
    "\n",
    "    #pickle the spectrogram test and train data\n",
    "    pickle.dump(spectrograms_test, open('pickle/spectrograms_test.pkl', 'wb'))\n",
    "    pickle.dump(spectrograms_train, open('pickle/spectrograms_train.pkl', 'wb'))\n",
    "    #pickle the labels\n",
    "    pickle.dump(labels_test, open('pickle/labels_test.pkl', 'wb'))\n",
    "    pickle.dump(labels_train, open('pickle/labels_train.pkl', 'wb'))\n",
    "else:\n",
    "    #load the spectrogram test and train data\n",
    "    spectrograms_test = pickle.load(open('pickle/spectrograms_test.pkl', 'rb'))\n",
    "    spectrograms_train = pickle.load(open('pickle/spectrograms_train.pkl', 'rb'))\n",
    "    #load the labels\n",
    "    labels_test = pickle.load(open('pickle/labels_test.pkl', 'rb'))\n",
    "    labels_train = pickle.load(open('pickle/labels_train.pkl', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Spectrograms shape: {spectrograms_train.shape}\")\n",
    "print(f\"Labels shape: {labels_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 96, 64)\n",
      "(10, 1, 96, 64)\n"
     ]
    }
   ],
   "source": [
    "#get the 2nd set of 10 speectrograms\n",
    "spec = spectrograms_train[0]\n",
    "\n",
    "SPEC_SHAPE = spec.shape\n",
    "print(SPEC_SHAPE)\n",
    "\n",
    "spec = spec.reshape(spec.shape[0], 1, spec.shape[1], spec.shape[2])\n",
    "print(spec.shape)\n",
    "\n",
    "#convert to tensor\n",
    "spec = torch.tensor(spec).float()\n",
    "\n",
    "embedding_model = vggish()\n",
    "embedding_model.eval()\n",
    "ex = embedding_model.forward(spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for reconstructing the log mel spectrogram without using griffin lim\n",
    "def reconstruct_log_mel_spectrogram(log_mel_spectrogram):\n",
    "    # transpose\n",
    "    log_mel_spectrogram = log_mel_spectrogram.T\n",
    "\n",
    "    # de-noramlize\n",
    "    log_mel_spectrogram = (log_mel_spectrogram * 255) + 255\n",
    "\n",
    "    # apply the inv log mel spectrogram\n",
    "    mel_spectrogram = librosa.db_to_amplitude(log_mel_spectrogram)\n",
    "\n",
    "    # apply the mel to audio\n",
    "    audio = librosa.feature.inverse.mel_to_audio(mel_spectrogram, sr=16000, n_fft=4000, hop_length=160, win_length=4000)\n",
    "\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import classification model with torch LOAD\n",
    "class_model = torch.load('model_2.pt')\n",
    "#make the class_model use cuda\n",
    "class_model = class_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionLevelSingleAttention(\n",
       "  (emb): EmbeddingLayers(\n",
       "    (conv1x1): ModuleList(\n",
       "      (0-2): 3 x Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (batchnorm): ModuleList(\n",
       "      (0-3): 4 x BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (attention): Attention(\n",
       "    (att): Conv2d(128, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (cla): Conv2d(128, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 128])\n",
      "tensor([[[0.6863, 0.2000, 0.8000,  ..., 0.6902, 0.6000, 1.0000],\n",
      "         [0.7647, 0.2471, 0.8275,  ..., 0.6627, 0.0000, 1.0000],\n",
      "         [0.8078, 0.2510, 1.0000,  ..., 0.0000, 0.0157, 1.0000],\n",
      "         ...,\n",
      "         [0.8039, 0.2627, 1.0000,  ..., 0.0000, 0.1804, 1.0000],\n",
      "         [0.7451, 0.1961, 0.8157,  ..., 0.7176, 0.0000, 1.0000],\n",
      "         [0.7294, 0.2118, 0.7843,  ..., 0.5765, 0.0000, 1.0000]]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ex = ex/255\n",
    "ex = ex.reshape((int(ex.shape[0]/10),10, ex.shape[1])).cuda()\n",
    "\n",
    "print(ex.shape)\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0818, 0.0898, 0.2421, 0.0501, 0.0278, 0.9153, 0.8577, 0.0453, 0.3478,\n",
      "         0.0891, 0.0157, 0.0112, 0.0658, 0.0335, 0.3674, 0.0303, 0.0297, 0.0068,\n",
      "         0.0596, 0.9633]], device='cuda:0', grad_fn=<HardtanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#classify the embedding\n",
    "class_model.eval()\n",
    "pred = class_model.forward(ex)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cymbal, drum, syth, voice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"accordion\": 0, \"banjo\": 1, \"bass\": 2, \"cello\": 3, \"clarinet\": 4, \"cymbals\": 5, \"drums\": 6, \n",
    " \"flute\": 7, \"guitar\": 8, \"mallet_percussion\": 9, \"mandolin\": 10, \"organ\": 11, \"piano\": 12, \"saxophone\": 13, \n",
    " \"synthesizer\": 14, \"trombone\": 15, \"trumpet\": 16, \"ukulele\": 17, \"violin\": 18, \"voice\": 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = nn.Conv2d(input.shape[1], num_filters, 3, padding=1)(input)\n",
    "    x = nn.BatchNorm2d(num_filters)(x)\n",
    "    x = nn.ReLU()(x)\n",
    "    x = nn.Conv2d(num_filters, num_filters, 3, padding=1)(x)\n",
    "    x = nn.BatchNorm2d(num_filters)(x)\n",
    "    x = nn.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = nn.MaxPool2d(2)(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = nn.ConvTranspose2d(input.shape[1], num_filters, 2, stride=2)(input)\n",
    "    x = torch.cat([x, skip_features], dim=1)\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_filters = 32\n",
    "# inst_models = {}\n",
    "\n",
    "\n",
    "# for i in range(20):\n",
    "    \n",
    "#     #make the input layer (1, 10, 96, 64)\n",
    "#     inputs = nn.Input((1, 10, 96, 64))\n",
    "\n",
    "#     # Encoder\n",
    "#     e1, p1 = encoder_block(inputs, num_filters)\n",
    "#     e2, p2 = encoder_block(p1, num_filters*2)\n",
    "#     e3, p3 = encoder_block(p2, num_filters*4)\n",
    "#     e4, p4 = encoder_block(p3, num_filters*8)\n",
    "\n",
    "#     # Bridge\n",
    "#     b1 = conv_block(p4, num_filters*16)\n",
    "\n",
    "#     # Decoder\n",
    "#     d1 = decoder_block(b1, e4, num_filters*8)\n",
    "#     d2 = decoder_block(d1, e3, num_filters*4)\n",
    "#     d3 = decoder_block(d2, e2, num_filters*2)\n",
    "#     d4 = decoder_block(d3, e1, num_filters)\n",
    "\n",
    "#     #unet output with sigmoid activation named 'unet_out' and the same shape as the input\n",
    "#     unet_out = nn.Conv2d(d4.shape[1], 1, 1, activation='sigmoid', name='unet_out')(d4)\n",
    "\n",
    "#     out_shaped = out_shaped.reshape(out_shaped.shape[0], 1, out_shaped.shape[1], out_shaped.shape[2])\n",
    "#     #run through vggish\n",
    "#     embedding_model = vggish()(out_shaped)\n",
    "\n",
    "#     outputs = class_model(embedding_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetSource(nn.Module):\n",
    "    def __init__(self,input_shape, num_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        # VGGish model\n",
    "        self.vggish = vggish()\n",
    "        #set trainable to false\n",
    "        for param in self.vggish.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Classifier model\n",
    "        self.class_model = torch.load('model_2.pt')\n",
    "        #set trainable to false\n",
    "        for param in self.class_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Encoder\n",
    "        self.input_shape = input_shape\n",
    "        self.e1, self.p1 = encoder_block(self.input_shape, num_filters)\n",
    "        self.e2, self.p2 = encoder_block(self.p1, num_filters*2)\n",
    "        self.e3, self.p3 = encoder_block(self.p2, num_filters*4)\n",
    "        self.e4, self.p4 = encoder_block(self.p3, num_filters*8)\n",
    "\n",
    "        # Bridge\n",
    "        self.b1 = conv_block(self.p4, num_filters*16)\n",
    "\n",
    "        # Decoder\n",
    "        self.d1 = decoder_block(self.b1, self.e4, num_filters*8)\n",
    "        self.d2 = decoder_block(self.d1, self.e3, num_filters*4)\n",
    "        self.d3 = decoder_block(self.d2, self.e2, num_filters*2)\n",
    "        self.d4 = decoder_block(self.d3, self.e1, num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # UNet\n",
    "        e1 = self.e1(x)\n",
    "        p1 = self.p1(e1)\n",
    "        e2 = self.e2(p1)\n",
    "        p2 = self.p2(e2)\n",
    "        e3 = self.e3(p2)\n",
    "        p3 = self.p3(e3)\n",
    "        e4 = self.e4(p3)\n",
    "        p4 = self.p4(e4)\n",
    "\n",
    "        b1 = self.b1(p4)\n",
    "\n",
    "        d1 = self.d1(b1, e4)\n",
    "        d2 = self.d2(d1, e3)\n",
    "        d3 = self.d3(d2, e2)\n",
    "        d4 = self.d4(d3, e1)\n",
    "\n",
    "        unet_out = nn.Conv2d(d4.shape[1], 1, 1, activation='sigmoid', name='unet_out')(d4)\n",
    "\n",
    "        masked_input = x * unet_out\n",
    "        \n",
    "        # VGGish\n",
    "        vgg_unet = unet_out.reshape(masked_input.shape[0], 1, masked_input.shape[1], masked_input.shape[2])\n",
    "\n",
    "        embedding_model = self.vggish(vgg_unet)\n",
    "\n",
    "        # Classifier\n",
    "        out = self.class_model(embedding_model)\n",
    "        \n",
    "        return masked_input, out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "p =input_example=torch.randn(1, 10, 96, 64)\n",
    "print(p.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ClassificationAndTraining(nn.Module):\n",
    "    def __init__(self, num_filters=32, num_classes=2, num_inst_models=20, input_example=torch.randn(1, 10, 96, 64)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_example = input_example\n",
    "        self.num_filters = num_filters\n",
    "        self.num_classes = num_classes\n",
    "        self.num_inst_models = num_inst_models\n",
    "\n",
    "        self.vggish = vggish()\n",
    "        #set trainable to false\n",
    "        for param in self.vggish.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "\n",
    "        # Classification network\n",
    "        self.classification = torch.load('model_2.pt')\n",
    "        #set trainable to false\n",
    "        for param in self.classification.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Instantiate UNetSource models\n",
    "        self.inst_models = nn.ModuleList([UNetSource(self.input_example, num_filters) for _ in range(num_inst_models)])\n",
    "        \n",
    "        # Define loss as categorical cross-entropy\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embed = self.vggish(x)\n",
    "        # Classify the input\n",
    "        classification_outputs = self.classification(embed)\n",
    "\n",
    "        # threshold the classification output\n",
    "        classification_outputs = torch.where(classification_outputs > 0.5, 1, 0)\n",
    "      \n",
    "        inst_outputs = {}\n",
    "\n",
    "        for inst in classification_outputs:\n",
    "            if inst == 1:\n",
    "                # Run through the corresponding instance model\n",
    "                unet_out, out = self.inst_models[inst](x)\n",
    "                #save both outputs and the instance model index\n",
    "                inst_outputs[inst] = (unet_out, out)\n",
    "\n",
    "        return inst_outputs, x\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training and validation data loaders\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(spectrograms_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(spectrograms_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationAndTraining(\n",
       "  (vggish): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (12): ReLU(inplace=True)\n",
       "      (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (14): ReLU(inplace=True)\n",
       "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (embeddings): Sequential(\n",
       "      (0): Linear(in_features=12288, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Linear(in_features=4096, out_features=128, bias=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classification): DecisionLevelSingleAttention(\n",
       "    (emb): EmbeddingLayers(\n",
       "      (conv1x1): ModuleList(\n",
       "        (0-2): 3 x Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (batchnorm): ModuleList(\n",
       "        (0-3): 4 x BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (att): Conv2d(128, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (cla): Conv2d(128, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (inst_models): ModuleList(\n",
       "    (0-19): 20 x UNetSource(\n",
       "      (vggish): VGG(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU(inplace=True)\n",
       "          (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (7): ReLU(inplace=True)\n",
       "          (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (9): ReLU(inplace=True)\n",
       "          (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (12): ReLU(inplace=True)\n",
       "          (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (14): ReLU(inplace=True)\n",
       "          (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (embeddings): Sequential(\n",
       "          (0): Linear(in_features=12288, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (3): ReLU(inplace=True)\n",
       "          (4): Linear(in_features=4096, out_features=128, bias=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (class_model): DecisionLevelSingleAttention(\n",
       "        (emb): EmbeddingLayers(\n",
       "          (conv1x1): ModuleList(\n",
       "            (0-2): 3 x Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (batchnorm): ModuleList(\n",
       "            (0-3): 4 x BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (attention): Attention(\n",
       "          (att): Conv2d(128, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (cla): Conv2d(128, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "source_sep_model = ClassificationAndTraining(num_filters=32)\n",
    "optimizer = optim.Adam(source_sep_model.parameters(), lr=0.001)\n",
    "source_sep_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1516126920\n",
      "Number of trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in source_sep_model.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in source_sep_model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNetSource(\n",
       "  (vggish): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (12): ReLU(inplace=True)\n",
       "      (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (14): ReLU(inplace=True)\n",
       "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (embeddings): Sequential(\n",
       "      (0): Linear(in_features=12288, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Linear(in_features=4096, out_features=128, bias=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (class_model): DecisionLevelSingleAttention(\n",
       "    (emb): EmbeddingLayers(\n",
       "      (conv1x1): ModuleList(\n",
       "        (0-2): 3 x Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (batchnorm): ModuleList(\n",
       "        (0-3): 4 x BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (att): Conv2d(128, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (cla): Conv2d(128, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_example = UNetSource(torch.randn(1, 10, 96, 64), num_filters=32)\n",
    "unet_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 72196520\n",
      "Number of trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in unet_example.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in unet_example.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[246], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m \u001b[39m# Forward pass the inputs through the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m outputs, in_spec \u001b[39m=\u001b[39m source_sep_model(inputs)\n\u001b[1;32m     14\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     16\u001b[0m sources \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/podElias/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[240], line 35\u001b[0m, in \u001b[0;36mClassificationAndTraining.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 35\u001b[0m     embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvggish(x\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     36\u001b[0m     \u001b[39m# Classify the input\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     classification_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification(embed)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "# Loop over the epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to train mode\n",
    "    source_sep_model.train()\n",
    "    \n",
    "    # Loop over the batches of data\n",
    "    for batch_idx, inputs in enumerate(train_loader):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass the inputs through the model\n",
    "        outputs, in_spec = source_sep_model(inputs)\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        sources = []\n",
    "\n",
    "        for inst in outputs.keys():\n",
    "\n",
    "            sources.append(outputs[inst][0])\n",
    "\n",
    "            #create a numpy array of the same size as the output with zeros everywhere except for the index of the instance\n",
    "            inst_tensor = torch.from_numpy(np.zeros(outputs[inst][1].shape))\n",
    "            inst_tensor[inst] = 1\n",
    "            labels_tensor = inst_tensor.type(torch.FloatTensor)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = source_sep_model.loss(outputs[inst][1], labels_tensor)\n",
    "            \n",
    "            # Backpropagate the loss and update the weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        mixture = torch.sum(torch.stack(sources), dim=0)\n",
    "\n",
    "        # reconstruct the log mel spectrogram mixture\n",
    "        mix_wav = reconstruct_log_mel_spectrogram(mixture.detach().numpy())\n",
    "        in_wav = reconstruct_log_mel_spectrogram(in_spec.detach().numpy())\n",
    "        \n",
    "        #calculate the signal to distortion ratio\n",
    "        sdr, _, _, _ = mir_eval.separation.bss_eval_sources(in_wav, mix_wav)\n",
    "  \n",
    "        # Print the loss and accuracy after each batch\n",
    "        print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\SDR: {:.6f} dB'.format(\n",
    "            epoch + 1, batch_idx * len(inputs), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), total_loss, sdr))\n",
    "        \n",
    "    # Set the model to eval mode\n",
    "    source_sep_model.eval()\n",
    "    \n",
    "    # Calculate the accuracy on the validation set\n",
    "    val_loss = 0\n",
    "    val_sdr = 0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs in val_loader:\n",
    "            # Forward pass the inputs through the model\n",
    "            val_outputs, _ = source_sep_model(val_inputs)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            val_sources = []\n",
    "\n",
    "            for val_inst in val_outputs.keys():\n",
    "\n",
    "                #create a numpy array of the same size as the output with zeros everywhere except for the index of the instance\n",
    "                val_inst_tensor = torch.from_numpy(np.zeros(val_outputs[val_inst][1].shape))\n",
    "                val_inst_tensor[val_inst] = 1\n",
    "                val_labels_tensor = val_inst_tensor.type(torch.FloatTensor)\n",
    "\n",
    "                val_loss += source_sep_model.loss(val_outputs[val_inst][1], val_labels_tensor).item()\n",
    "\n",
    "                val_sources.append(val_outputs[val_inst][0])\n",
    "\n",
    "\n",
    "            val_mixture = torch.sum(torch.stack(val_sources), dim=0)\n",
    "\n",
    "            # reconstruct the log mel spectrogram mixture\n",
    "            val_mix_wav = reconstruct_log_mel_spectrogram(val_mixture.detach().numpy())\n",
    "            val_in_wav = reconstruct_log_mel_spectrogram(val_inputs.detach().numpy())\n",
    "            \n",
    "            #calculate the signal to distortion ratio\n",
    "            curr_val_sdr, _, _, _ = mir_eval.separation.bss_eval_sources(val_in_wav, val_mix_wav)\n",
    "            val_sdr += curr_val_sdr\n",
    "            \n",
    "            \n",
    "    \n",
    "    # Print the validation loss and accuracy\n",
    "    print('Validation set: Average loss: {:.4f}, SDR: {}/{} ({:.0f}dB)\\n'.format(\n",
    "        val_loss / len(val_loader.dataset), val_sdr, len(val_loader.dataset),\n",
    "        val_sdr / len(val_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
